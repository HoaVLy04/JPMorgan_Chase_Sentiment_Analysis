{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01021d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b70b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_df = pd.read_csv('complaints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4479470",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_df['Date received'] = pd.to_datetime(complaints_df['Date received'], format='%m/%d/%Y', errors='coerce')\n",
    "complaints_df['Date sent to company'] = pd.to_datetime(complaints_df['Date sent to company'], format='%m/%d/%Y', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a157e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Tags' in complaints_df.columns:\n",
    "    complaints_df['Tags'] = complaints_df['Tags'].str.split(', ')\n",
    "    complaints_df = complaints_df.explode('Tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_df = complaints_df.replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in complaints_df.select_dtypes(include='object').columns:\n",
    "    complaints_df[col] = complaints_df[col].replace('N/A', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85957ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = complaints_df.nunique()\n",
    "print(\"\\nUnique value counts per column:\")\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_companies = complaints_df['Company'].value_counts().head(5)\n",
    "print(\"\\nTop 5 companies with the most records:\")\n",
    "print(top_5_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d917082",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Consumer complaint narrative' not in complaints_df.columns:\n",
    "    print(\"\\nError: 'Consumer complaint narrative' column not found. Cannot proceed with sentiment analysis.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a2b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpm_complaints_df = complaints_df[complaints_df['Company'] == \"JPMORGAN CHASE & CO.\"].copy()\n",
    "jpm_complaints_df['tokenized_narrative'] = jpm_complaints_df['Consumer complaint narrative'].astype(str).apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1471bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpm_complaints_df['sentiment_scores'] = jpm_complaints_df['Consumer complaint narrative'].astype(str).apply(sid.polarity_scores)\n",
    "jpm_complaints_df['compound_sentiment'] = jpm_complaints_df['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "jpm_complaints_df['negative_sentiment'] = jpm_complaints_df['sentiment_scores'].apply(lambda x: x['neg'])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "negative_words_list = []\n",
    "for index, row in jpm_complaints_df.iterrows():\n",
    "    if row['negative_sentiment'] > 0.5: # A threshold for 'highly negative'\n",
    "        tokens = [word.lower() for word in row['tokenized_narrative'] if word.isalpha() and word.lower() not in stop_words]\n",
    "        negative_words_list.extend(tokens)\n",
    "negative_word_count = Counter(negative_words_list) # Top 50 negative words\n",
    "print(\"\\nTop negative words for JPMorgan Chase & Co. (based on VADER):\")\n",
    "print(negative_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e2492",
   "metadata": {},
   "outputs": [],
   "source": [
    "if negative_word_count:\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(negative_word_count))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Negative Word Cloud for JPMorgan Chase & Co. Complaints')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No sufficiently negative words found to generate a word cloud.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc6b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_sentiment = jpm_complaints_df.groupby('Product')['compound_sentiment'].mean().reset_index()\n",
    "product_sentiment = product_sentiment.sort_values(by='compound_sentiment', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x='Product', y='compound_sentiment', data=product_sentiment, palette='viridis')\n",
    "plt.title('Average Compound Sentiment by Product for JPMorgan Chase & Co.')\n",
    "plt.xlabel('Product')\n",
    "plt.ylabel('Average Compound Sentiment')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels for plotting\n",
    "emotion_by_dispute['Complaint Status'] = emotion_by_dispute['Consumer disputed'].map({'No': 'Not Disputed', 'Yes': 'Disputed'})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=emotion_by_dispute,\n",
    "    x='sentiment',\n",
    "    y='avg_proportion',\n",
    "    hue='Complaint Status',\n",
    "    palette={'Not Disputed': '#E74C3C', 'Disputed': '#3498DB'}\n",
    ")\n",
    "\n",
    "plt.title(\"Emotional Content in Disputed vs. Non-Disputed Complaints\\nComparison of normalized emotion scores\")\n",
    "plt.xlabel(\"Emotion\")\n",
    "plt.ylabel(\"Average Proportion of Words\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Complaint Status')\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dff8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming emotion_wide is a pandas DataFrame and 'binary_dispute' is the target column\n",
    "X = emotion_wide[['anger', 'fear', 'joy', 'sadness', 'trust', 'surprise', 'anticipation', 'disgust']]\n",
    "X = sm.add_constant(X)  # Adds the intercept\n",
    "y = emotion_wide['binary_dispute']\n",
    "\n",
    "# Fit logistic regression model\n",
    "dispute_model = sm.Logit(y, X).fit()\n",
    "\n",
    "# To view a summary like R's summary()\n",
    "print(dispute_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de35a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary, which includes the likelihood ratio test\n",
    "print(dispute_model.summary())\n",
    "\n",
    "# Or extract likelihood ratio test statistic and p-value\n",
    "lrt_stat = dispute_model.llr\n",
    "lrt_pvalue = dispute_model.llr_pvalue\n",
    "\n",
    "print(\"Likelihood Ratio Chi-Square Statistic:\", lrt_stat)\n",
    "print(\"p-value:\", lrt_pvalue)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
